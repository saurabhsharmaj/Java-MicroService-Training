apiVersion: v1
kind: Namespace
metadata:
  name: observability

---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: kube-state-metrics
  namespace: observability
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: kube-state-metrics
rules:
  - apiGroups: [""]
    resources:
      - nodes
      - pods
      - services
      - resourcequotas
      - replicationcontrollers
      - limitranges
      - persistentvolumeclaims
      - persistentvolumes
      - namespaces
    verbs: ["list", "watch"]
  - apiGroups: ["apps"]
    resources:
      - statefulsets
      - daemonsets
      - deployments
      - replicasets
    verbs: ["list", "watch"]
  - apiGroups: ["batch"]
    resources:
      - cronjobs
      - jobs
    verbs: ["list", "watch"]
  - apiGroups: ["autoscaling"]
    resources:
      - horizontalpodautoscalers
    verbs: ["list", "watch"]
  - apiGroups: ["policy"]
    resources:
      - poddisruptionbudgets
    verbs: ["list", "watch"]
  - apiGroups: ["storage.k8s.io"]
    resources:
      - storageclasses
      - volumeattachments
    verbs: ["list", "watch"]
  - apiGroups: ["networking.k8s.io"]
    resources:
      - networkpolicies
    verbs: ["list", "watch"]
  - apiGroups: ["certificates.k8s.io"]
    resources:
      - certificatesigningrequests
    verbs: ["list", "watch"]
  - apiGroups: ["authorization.k8s.io"]
    resources:
      - subjectaccessreviews
    verbs: ["create"]
  - apiGroups: ["rbac.authorization.k8s.io"]
    resources:
      - roles
      - rolebindings
      - clusterroles
      - clusterrolebindings
    verbs: ["list", "watch"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: kube-state-metrics
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: kube-state-metrics
subjects:
  - kind: ServiceAccount
    name: kube-state-metrics
    namespace: observability
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: kube-state-metrics
  namespace: observability
  labels:
    app: kube-state-metrics
spec:
  replicas: 1
  selector:
    matchLabels:
      app: kube-state-metrics
  template:
    metadata:
      labels:
        app: kube-state-metrics
    spec:
      serviceAccountName: kube-state-metrics
      containers:
        - name: kube-state-metrics
          image: registry.k8s.io/kube-state-metrics/kube-state-metrics:v2.10.1
          ports:
            - name: http-metrics
              containerPort: 8080
            - name: telemetry
              containerPort: 8081

---
apiVersion: v1
kind: Service
metadata:
  name: kube-state-metrics
  namespace: observability
spec:
  type: NodePort
  selector:
    app: kube-state-metrics
  ports:
    - name: http-metrics
      port: 8080
      targetPort: http-metrics
      nodePort: 30082  # optional: choose a port in range 30000–32767
    - name: telemetry
      port: 8081
      targetPort: telemetry
      nodePort: 30083  # optional
---

apiVersion: v1
kind: ConfigMap
metadata:
  name: loki-config
  namespace: observability
data:
  loki.yaml: |
    auth_enabled: false

    server:
      http_listen_port: 3100

    common:
      instance_addr: 0.0.0.0
      path_prefix: /loki
      storage:
        filesystem:
          chunks_directory: /loki/chunks
          rules_directory: /loki/rules
      replication_factor: 1
      ring:
        kvstore:
          store: inmemory

    schema_config:
      configs:
        - from: 2020-10-24
          store: tsdb
          object_store: filesystem
          schema: v13
          index:
            prefix: index_
            period: 24h

    ruler:
      alertmanager_url: http://alertmanager:9093
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: loki
  namespace: observability
  labels:
    app: loki
spec:
  replicas: 1
  selector:
    matchLabels:
      app: loki
  template:
    metadata:
      labels:
        app: loki
    spec:
      containers:
        - name: loki
          image: grafana/loki:2.9.0
          args:
            - -config.file=/etc/loki/loki.yaml
          ports:
            - containerPort: 3100
          volumeMounts:
            - name: loki-config
              mountPath: /etc/loki
            - name: loki-storage
              mountPath: /loki
      volumes:
        - name: loki-config
          configMap:
            name: loki-config
        - name: loki-storage
          emptyDir: {}
---
apiVersion: v1
kind: Service
metadata:
  name: loki
  namespace: observability
spec:
  selector:
    app: loki
  ports:
    - protocol: TCP
      port: 3100
      targetPort: 3100
  type: ClusterIP  

---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: promtail
  namespace: observability
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: promtail
rules:
  - apiGroups: [""]
    resources: ["pods", "nodes", "namespaces"]
    verbs: ["get", "list", "watch"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: promtail
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: promtail
subjects:
  - kind: ServiceAccount
    name: promtail
    namespace: observability

---
apiVersion: v1
kind: ConfigMap
metadata:
  name: promtail-config
  namespace: observability
data:
  promtail.yaml: |
    server:
      http_listen_port: 9080
      grpc_listen_port: 0

    positions:
      filename: /tmp/positions.yaml

    clients:
  
      # - url: http://10.108.102.21:3100/loki/api/v1/push
       - url: http://loki.observability.svc:3100/loki/api/v1/push

    scrape_configs:
      - job_name: kubernetes-pods
        kubernetes_sd_configs:
          - role: pod
        pipeline_stages:
          - cri: {}
        relabel_configs:
          - source_labels: [__meta_kubernetes_namespace]
            target_label: namespace
          - source_labels: [__meta_kubernetes_pod_name]
            target_label: pod
          - source_labels: [__meta_kubernetes_pod_container_name]
            target_label: container
          - source_labels: [__meta_kubernetes_pod_node_name]
            target_label: node
          - source_labels: [__meta_kubernetes_pod_label_app]
            target_label: app
          - action: replace
            source_labels:
              - __meta_kubernetes_namespace
              - __meta_kubernetes_pod_name
              - __meta_kubernetes_pod_container_name
            regex: (.*);(.*);(.*)
            replacement: /var/log/pods/$1_$2*/$3/*.log
            target_label: __path__

      - job_name: varlogs
        static_configs:
           - targets:
             - localhost
             labels:
                job: varlogs
                __path__: /var/log/containers/*.log        
              
---
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: promtail
  namespace: observability
  labels:
    app: promtail
spec:
  selector:
    matchLabels:
      app: promtail
  template:
    metadata:
      labels:
        app: promtail
    spec:
      serviceAccountName: promtail
      containers:
        - name: promtail
          image: grafana/promtail:2.9.0
          args:
            - -config.file=/etc/promtail/promtail.yaml
          volumeMounts:
            - name: config
              mountPath: /etc/promtail
            - name: varlogpods
              mountPath: /var/log/pods
            - name: varlogcontainers
              mountPath: /var/log/containers
            - name: positions
              mountPath: /tmp
          resources:
            limits:
              memory: 200Mi
              cpu: 100m
      volumes:
        - name: config
          configMap:
            name: promtail-config
        - name: varlogpods
          hostPath:
            path: /var/log/pods
        - name: varlogcontainers
          hostPath:
            path: /var/log/containers
        - name: positions
          emptyDir: {}

---
apiVersion: v1
kind: Service
metadata:
  name: grafana
  namespace: observability
spec:
  selector:
    app: grafana
  type: NodePort  # Exposing as NodePort
  ports:
    - name: http
      port: 3000
      targetPort: 3000
      nodePort: 30001  # NodePort in the range 30000–32767

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: grafana
  namespace: observability
  labels:
    app: grafana
spec:
  replicas: 1
  selector:
    matchLabels:
      app: grafana
  template:
    metadata:
      labels:
        app: grafana
    spec:
      containers:
        - name: grafana
          image: grafana/grafana
          ports:
            - containerPort: 3000
          env:
            - name: GF_SECURITY_ADMIN_PASSWORD
              value: "admin"
                #  - name: GF_INSTALL_PLUGINS
              #              value: "https://storage.googleapis.com/integration-artifacts/grafana-exploretraces-app/grafana-exploretraces-app-latest.zip;grafana-traces-app"

          volumeMounts:
               - name: grafana-datasources
                 mountPath: /etc/grafana/provisioning/datasources
               - name: grafana-dashboards
                 mountPath: /etc/grafana/provisioning/dashboards
                   # - name: dashboard-json
                 #  mountPath: /var/lib/grafana/dashboards
      volumes:
        - name: grafana-datasources
          configMap:
            name: grafana-datasources
        - name: grafana-dashboards
          configMap:
            name: grafana-dashboards
              # - name: dashboard-json
          #        configMap:
          # name: grafana-dashboard-json
   # configMap:
              # name: grafana-dashboard-json 
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: grafana-datasources
  namespace: observability
data:
  datasource.yaml: |
    apiVersion: 1
    datasources:
      - name: Loki
        type: loki
        url: http://loki.observability.svc:3100
        access: proxy
        isDefault: true
      - name: Prometheus
        type: prometheus
        url: http://prometheus.observability.svc:9090
        access: proxy
        isDefault: false
      - name: kube-state-metrics
        type: prometheus
        url: http://prometheus.observability.svc:9090
        access: proxy
        isDefault: false
      - name: Tempo
        type: tempo
        url: http://tempo.observability.svc:3200
        access: proxy
        isDefault: false


---
apiVersion: v1
kind: ConfigMap
metadata:
  name: grafana-dashboards
  namespace: observability
data:
  dashboard.yaml: |
    apiVersion: 1
    providers:
      - name: 'default'
        orgId: 1
        folder: ''
        type: file
        disableDeletion: false
        updateIntervalSeconds: 10
        options:
          path: /etc/grafana/provisioning/dashboards
---
apiVersion: v1
kind: Service
metadata:
  name: prometheus
  namespace: observability
spec:
  selector:
    app: prometheus
  ports:
     - name: http-metrics
       port: 9090
       targetPort: 9090
       nodePort: 30090 
  type: NodePort
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: prometheus
  namespace: observability
spec:
  replicas: 1
  selector:
    matchLabels:
      app: prometheus
  template:
    metadata:
      labels:
        app: prometheus
    spec:
      containers:
        - name: prometheus
          image: prom/prometheus:latest
          args:
            - "--config.file=/etc/prometheus/prometheus.yml"
          ports:
            - containerPort: 9090
          volumeMounts:
            - name: config
              mountPath: /etc/prometheus
      volumes:
        - name: config
          configMap:
            name: prometheus-config
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-config
  namespace: observability
data:
  prometheus.yml: |
    global:
      scrape_interval: 15s
    scrape_configs:
      - job_name: 'kubernetes-pods'
        kubernetes_sd_configs:
          - role: pod
      - job_name: 'kube-state-metrics'
        static_configs:
          - targets: ['kube-state-metrics.observability.svc:8080']
      - job_name: 'spring-boot-app'
        metrics_path: '/actuator/prometheus'
        static_configs:
          - targets: ['spring-boot-service.observability.svc:30506']  # If app is on host, and Prometheus in Docker
            labels:
              application: 'api-tutorial'

      - job_name: 'kubelet'
        scheme: https
        tls_config:
           insecure_skip_verify: true
        metrics_path: /metrics/cadvisor
        static_configs:
          - targets:
            - 192.168.1.8:10250
            - 192.168.1.34:10250
      - job_name: 'flask-app'
        static_configs:
           - targets: ['flask-app-service.observability.svc:5000']  # Replace with the actual IP/hostname of your Flask app

      - job_name: 'blackbox-exporter-metrics'
        metrics_path: /probe
        params:
          module: [http_endpoint]
        static_configs:
          - targets:
            - http://flask-app-service.observability.svc:5000
        relabel_configs:
         - source_labels: [__address__]
           target_label: __param_target
         - source_labels: [__param_target]
           target_label: instance
         - target_label: __address__
           replacement: "blackbox-exporter.observability.svc:9115"        


      - job_name: 'node-exporter'
        static_configs:
           - targets: ['node-exporter.observability.svc:9100'] # Add all node IPs

     
      - job_name: 'tempo'
        static_configs:
           - targets: ['tempo.observability.svc:3200']
        
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: tempo-config
  namespace: observability
data:
  tempo.yaml: |
    server:
      http_listen_port: 3200
    distributor:
      receivers:
        jaeger:
          protocols:
            thrift_http:
              endpoint: 0.0.0.0:14268
            grpc: {}
            thrift_binary: {}
            thrift_compact: {}
        otlp:
          protocols:
            grpc:
              endpoint: 0.0.0.0:4317
            http:
              endpoint: 0.0.0.0:4318
    storage:
      trace:
        backend: local
        local:
          path: /var/tempo
        wal:
          path: /var/tempo/wal
    memberlist:
      join_members:
        - tempo.observability.svc.cluster.local
    metrics_generator:
      storage:
        path: /var/tempo/metrics  # Enable metrics generation by setting the storage path
    # Prometheus Metrics Configuration (expose on port 9090)
    #    metrics:
    #  http:
    #    listen_address: ":9090"  # Expose Prometheus metrics on port 9090

---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: tempo
  namespace: observability
spec:
  serviceName: tempo
  replicas: 1
  selector:
    matchLabels:
      app: tempo
  template:
    metadata:
      labels:
        app: tempo
    spec:
      containers:
        - name: tempo
          image: grafana/tempo:latest
          args: ["-config.file=/etc/tempo/tempo.yaml"]
          ports:
            - name: http
              containerPort: 3200
            - name: jaeger-thrift
              containerPort: 6831
            - name: jaeger-compact
              containerPort: 6832
            - name: jaeger-grpc
              containerPort: 14250
            - name: jaeger-http
              containerPort: 14268
            - name: otlp-grpc
              containerPort: 4317
            - name: otlp-http
              containerPort: 4318
            - name: metrics
              containerPort: 9090  # Expose Prometheus metrics
          volumeMounts:
            - name: config
              mountPath: /etc/tempo
            - name: storage
              mountPath: /var/tempo
      volumes:
        - name: config
          configMap:
            name: tempo-config
        - name: storage
          emptyDir: {}

---
apiVersion: v1
kind: Service
metadata:
  name: tempo
  namespace: observability
spec:
  selector:
    app: tempo
  ports:
    - name: http
      port: 3200
      targetPort: http
    - name: jaeger-thrift
      port: 6831
      targetPort: jaeger-thrift
    - name: jaeger-compact
      port: 6832
      targetPort: jaeger-compact
    - name: jaeger-grpc
      port: 14250
      targetPort: jaeger-grpc
    - name: jaeger-http
      port: 14268
      targetPort: jaeger-http
    - name: otlp-grpc
      port: 4317
      targetPort: otlp-grpc
    - name: otlp-http
      port: 4318
      targetPort: otlp-http
    - name: metrics
      port: 9090
      targetPort: metrics  # Expose Prometheus metrics on port 9090
  type: ClusterIP

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mysql
  namespace: observability
spec:
  replicas: 1
  selector:
    matchLabels:
      app: mysql
  template:
    metadata:
      labels:
        app: mysql
    spec:
      containers:
        - name: mysql
          image: mysql:8
          env:
            - name: MYSQL_ROOT_PASSWORD
              value: root
            - name: MYSQL_DATABASE
              value: user_database
          ports:
            - containerPort: 3306
---
apiVersion: v1
kind: Service
metadata:
  name: mysql
  namespace: observability
spec:
  selector:
    app: mysql
  ports:
    - port: 3306
      targetPort: 3306
  type: ClusterIP

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: api-tutorial
  namespace: observability
spec:
  replicas: 1
  selector:
    matchLabels:
      app: api-tutorial
  template:
    metadata:
      labels:
        app: api-tutorial
    spec:
      containers:
        - name: app
          image: rajdeepsingh642/java-app:v1  # replace with your image
          ports:
            - containerPort: 8082
          env:
            - name: OTEL_SERVICE_NAME
              value: api-tutorial
            - name: OTEL_EXPORTER_OTLP_ENDPOINT
              value: http://tempo.observability.svc:4317
            - name: OTEL_EXPORTER_OTLP_PROTOCOL
              value: grpc
            - name: OTEL_METRICS_EXPORTER
              value: none
            - name: SPRING_DATASOURCE_URL
              value: jdbc:mysql://mysql.observability.svc:3306/user_database?allowPublicKeyRetrieval=true&useSSL=false
            - name: SPRING_DATASOURCE_USERNAME
              value: root
            - name: SPRING_DATASOURCE_PASSWORD
              value: root
---
apiVersion: v1
kind: Service
metadata:
  name: api-tutorial
  namespace: observability
spec:
  selector:
    app: api-tutorial
  ports:
    - port: 8082
      targetPort: 8082
  type: NodePort
